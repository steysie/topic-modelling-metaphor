{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# import joblib\n",
    "import pickle\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from varname import nameof\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for classification and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, we tried 11 different classifiers, but eventually decided to report scores only for the top 3: Logistic Regression, Linear SVM and Neural Network. Uncomment lines with classifiers and their names if you wou want to try more than three classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \n",
    "         #\"Logisitic Regression SGD\", \n",
    "         \"Linear SVM\", \n",
    "         #\"RBF SVM\", \n",
    "         #\"Naive Bayes\", \n",
    "         #\"Gaussian Process\", \n",
    "         #\"Decision Tree\", \n",
    "         #\"Random Forest\", \n",
    "         \"Neural Net\", \n",
    "         #\"AdaBoost\", \n",
    "         #\"Nearest Heighbors\"\n",
    "         ]\n",
    "\n",
    "classifiers = [LogisticRegression(class_weight='balanced', solver='liblinear', fit_intercept=True, max_iter=10000),\n",
    "               #linear_model.SGDClassifier(max_iter=50000, tol=1e-3, loss='log', class_weight='balanced'),\n",
    "               SVC(kernel=\"linear\", C=0.025, max_iter=10000),\n",
    "               #SVC(gamma=2), \n",
    "               #GaussianNB(),\n",
    "               #GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "               #DecisionTreeClassifier(max_depth=10),\n",
    "               #RandomForestClassifier(max_depth=10, n_estimators=10),\n",
    "               MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive'),\n",
    "               #AdaBoostClassifier(),\n",
    "               #KNeighborsClassifier(n_neighbors=2)\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, k_fold=5):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    train_results = {}    # {'clf': {'accuracy':[], 'precision':[], 'recall':[], 'f1':[])}\n",
    "\n",
    "    #kf = KFold(k_fold, shuffle=True, random_state=42)\n",
    "    kf = StratifiedKFold(k_fold, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_num = 1\n",
    "    saved_models = {}\n",
    "\n",
    "    for train_ind, val_ind in kf.split(X, y):\n",
    "        # Assign CV IDX\n",
    "        X_train, y_train = X[train_ind], y[train_ind]\n",
    "        X_val, y_val = X[val_ind], y[val_ind]\n",
    "        \n",
    "        # Scale Data\n",
    "        # scaler = StandardScaler()\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scale = scaler.fit_transform(X_train)\n",
    "        X_val_scale = scaler.transform(X_val)\n",
    "        \n",
    "        # print(\"Fold num: \", fold_num)\n",
    "\n",
    "        for name, clf in zip(names, classifiers):\n",
    "\n",
    "            if name not in train_results:\n",
    "                train_results[name] = {'accuracy':[], 'precision':[], \n",
    "                                       'recall':[], 'f1':[]} # 'train_time':[]}\n",
    "\n",
    "            # print(\"Training: \", name)\n",
    "            #start_time = time.time()\n",
    "\n",
    "            model = clf.fit(X_train_scale, y_train)\n",
    "            y_pred = model.predict(X_val_scale)\n",
    "            \n",
    "            # save the model\n",
    "#             joblib.dump(model, '{}{}.joblib'.format('_'.join(name.lower().split()), \n",
    "#                                                     model_save_postfix))\n",
    "            saved_models[name] = pickle.dumps(model)\n",
    "\n",
    "            train_results[name]['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "            train_results[name]['precision'].append(precision_score(y_val, y_pred))\n",
    "            train_results[name]['recall'].append(recall_score(y_val, y_pred))\n",
    "            train_results[name]['f1'].append(f1_score(y_val, y_pred))\n",
    "            #train_results[name]['train_time'].append(time.time() - start_time)\n",
    "\n",
    "            #print(\"Run Time: \", time.time() - start_time)\n",
    "        \n",
    "        fold_num += 1\n",
    "        # print()\n",
    "    \n",
    "    return train_results, saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " def scores(results, mode='print'):   \n",
    "    '''Print or return metric report for all tested classifiers\n",
    "    '''\n",
    "    if mode == 'return':\n",
    "        result = \"\"\n",
    "        for clf, scores in results.items():\n",
    "            result += \"Scores for {}\\n\".format(clf)\n",
    "            result += f\"\\t Train accuracy: {np.mean(scores['accuracy']):.3f} +- {np.std(scores['accuracy']):.3f}\\n\"\n",
    "            result += f\"\\t Train precision: {np.mean(scores['precision']):.3f} +- {np.std(scores['precision']):.3f}\\n\"\n",
    "            result += f\"\\t Train recall: {np.mean(scores['recall']):.3f} +- {np.std(scores['recall']):.3f}\\n\"\n",
    "            result += f\"\\t Train f1-score: {np.mean(scores['f1']):.3f} +- {np.std(scores['f1']):.3f}\\n\"\n",
    "        return result\n",
    "        \n",
    "    elif mode == 'print':\n",
    "        for clf, scores in results.items():\n",
    "\n",
    "            print(\"Scores for \", clf)\n",
    "            print(f\"\\t Train accuracy: {np.mean(scores['accuracy']):.3f} +- {np.std(scores['accuracy']):.3f}\")\n",
    "            print(f\"\\t Train precision: {np.mean(scores['precision']):.3f} +- {np.std(scores['precision']):.3f}\")\n",
    "            print(f\"\\t Train recall: {np.mean(scores['recall']):.3f} +- {np.std(scores['recall']):.3f}\")\n",
    "            print(f\"\\t Train f1-score: {np.mean(scores['f1']):.3f} +- {np.std(scores['f1']):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_log(model_name, results):\n",
    "    # Print metric report for all tested classifiers\n",
    "    with open(model_name+\"train.txt\", \"w\") as f:\n",
    "        print(model_name, file=f)\n",
    "        for clf, scores in results.items():\n",
    "            print(\"Scores for \", clf, file=f)\n",
    "            print(f\"\\t Train accuracy: {np.mean(scores['accuracy']):.3f} +- {np.std(scores['accuracy']):.3f}\", file=f)\n",
    "            print(f\"\\t Train precision: {np.mean(scores['precision']):.3f} +- {np.std(scores['precision']):.3f}\", file=f)\n",
    "            print(f\"\\t Train recall: {np.mean(scores['recall']):.3f} +- {np.std(scores['recall']):.3f}\", file=f)\n",
    "            print(f\"\\t Train f1-score: {np.mean(scores['f1']):.3f} +- {np.std(scores['f1']):.3f}\", file=f)\n",
    "            #print(f\"\\t Train time: {np.sum(scores['train_time']):.3f}\", file=f)\n",
    "#save_log(model_name, train_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset with Topic Features from ARTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sents</th>\n",
       "      <th>targets</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_30</th>\n",
       "      <th>topic_31</th>\n",
       "      <th>topic_32</th>\n",
       "      <th>topic_33</th>\n",
       "      <th>topic_34</th>\n",
       "      <th>topic_35</th>\n",
       "      <th>topic_36</th>\n",
       "      <th>topic_37</th>\n",
       "      <th>topic_38</th>\n",
       "      <th>topic_39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>нужно_PRED весь_ADJF время_NOUN бомбардировать...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.002279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>добрынин_NOUN говорить_VERB шевченко_NOUN цент...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.094519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.014016</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>0.000974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>принять_INFN внимание_NOUN настойчиво_ADVB гру...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.490057</td>\n",
       "      <td>0.313551</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              sents  targets  \\\n",
       "0           0  нужно_PRED весь_ADJF время_NOUN бомбардировать...        1   \n",
       "1           1  добрынин_NOUN говорить_VERB шевченко_NOUN цент...        1   \n",
       "2           2  принять_INFN внимание_NOUN настойчиво_ADVB гру...        1   \n",
       "\n",
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  ...  \\\n",
       "0  0.002155  0.002220  0.003529  0.002120  0.002342  0.002089  0.002102  ...   \n",
       "1  0.025325  0.000967  0.001266  0.001371  0.000952  0.001201  0.094519  ...   \n",
       "2  0.000380  0.000474  0.012164  0.000360  0.001321  0.000927  0.000465  ...   \n",
       "\n",
       "   topic_30  topic_31  topic_32  topic_33  topic_34  topic_35  topic_36  \\\n",
       "0  0.002093  0.002011  0.002155  0.002518  0.001852  0.001864  0.001913   \n",
       "1  0.001370  0.001080  0.001069  0.002011  0.010275  0.033420  0.000900   \n",
       "2  0.001897  0.000829  0.000421  0.000422  0.000352  0.000353  0.000398   \n",
       "\n",
       "   topic_37  topic_38  topic_39  \n",
       "0  0.002302  0.003006  0.002279  \n",
       "1  0.014016  0.093960  0.000974  \n",
       "2  0.490057  0.313551  0.002572  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of topics (successful ones): 40, 50, 80, 90\n",
    "df = pd.read_csv('./theta_matrices/metcorp_tm_lda40.csv', index_col=None)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7077, 7077)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, 3:53].values.tolist()\n",
    "y = df['targets']\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for  Logistic Regression\n",
      "\t Train accuracy: 0.699 +- 0.008\n",
      "\t Train precision: 0.691 +- 0.008\n",
      "\t Train recall: 0.721 +- 0.018\n",
      "\t Train f1-score: 0.705 +- 0.010\n",
      "Scores for  Linear SVM\n",
      "\t Train accuracy: 0.670 +- 0.006\n",
      "\t Train precision: 0.631 +- 0.004\n",
      "\t Train recall: 0.819 +- 0.015\n",
      "\t Train f1-score: 0.713 +- 0.007\n",
      "Scores for  Neural Net\n",
      "\t Train accuracy: 0.698 +- 0.008\n",
      "\t Train precision: 0.691 +- 0.009\n",
      "\t Train recall: 0.715 +- 0.015\n",
      "\t Train f1-score: 0.703 +- 0.009\n"
     ]
    }
   ],
   "source": [
    "train_results, _ = train(X, y)\n",
    "scores(train_results)\n",
    "save_log(model_name=\"Metcorp_clf_sklearn\", results=train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_n = [40, 50, 80, 90]\n",
    "path = './theta_matrices/'\n",
    "lda_name = 'metcorp_tm_lda'\n",
    "dense_name = 'metcorp_tm_dense_'\n",
    "sparse_name = 'metcorp_tm_sparse_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training models for 40 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 50 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 80 topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steysie/.local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 90 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 40 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 50 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 80 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 90 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 40 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 50 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 80 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n",
      "Started training models for 90 topics\n",
      "Started predicting with Logistic Regression model\n",
      "done.\n",
      "Started predicting with Linear SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Started predicting with Neural Net model\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# saving prediction results for TM-based models\n",
    "for tm in [lda_name, sparse_name, dense_name]:\n",
    "\n",
    "    for t_n in topic_n:\n",
    "          \n",
    "        df = pd.read_csv('{}{}{}.csv'.format(path, tm, t_n), index_col=None)\n",
    "        X = df.iloc[:, 3:].values.tolist()\n",
    "        y = df['targets']\n",
    "        \n",
    "        print(\"Started training models for {} topics\".format(t_n))\n",
    "        _, models = train(X, y)\n",
    "        \n",
    "        results_table = df[['sents', 'targets']]\n",
    "        \n",
    "        for mname, m in models.items():\n",
    "            print(\"Started predicting with {} model\".format(mname))\n",
    "            \n",
    "            model = pickle.loads(m)\n",
    "            pred = model.predict(X)\n",
    "            results_table[mname] = pred\n",
    "            \n",
    "            print(\"done.\")\n",
    "            \n",
    "        results_table.to_csv(\"./clf_model_outputs/clf-res_{}_{}.csv\".format(tm.split('_')[2],\n",
    "                                                                            t_n),\n",
    "                            index=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clf_model_outputs/clf-res_lda_40.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sents</th>\n",
       "      <th>targets</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Linear SVM</th>\n",
       "      <th>Neural Net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>нужно_PRED весь_ADJF время_NOUN бомбардировать...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>добрынин_NOUN говорить_VERB шевченко_NOUN цент...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>принять_INFN внимание_NOUN настойчиво_ADVB гру...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>кроме_PREP покупка_NOUN рука_NOUN сохранять_VE...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>сигнал_NOUN настойчиво_ADVB бомбардировать_INF...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sents  targets  \\\n",
       "0  нужно_PRED весь_ADJF время_NOUN бомбардировать...        1   \n",
       "1  добрынин_NOUN говорить_VERB шевченко_NOUN цент...        1   \n",
       "2  принять_INFN внимание_NOUN настойчиво_ADVB гру...        1   \n",
       "3  кроме_PREP покупка_NOUN рука_NOUN сохранять_VE...        1   \n",
       "4  сигнал_NOUN настойчиво_ADVB бомбардировать_INF...        1   \n",
       "\n",
       "   Logistic Regression  Linear SVM  Neural Net  \n",
       "0                    0           1           0  \n",
       "1                    1           1           1  \n",
       "2                    1           1           1  \n",
       "3                    1           1           1  \n",
       "4                    0           0           0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk train on multiple ARTM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_stdin():\n",
    "    if hasattr(tqdm, '_instances'):\n",
    "        for instance in list(tqdm._instances):\n",
    "            tqdm._decr_instances(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_train(theta_table_names, theta_dir='', save_log_name='bulk_train_output.txt'):\n",
    "    '''\n",
    "    Train and evaluate classifiers on different feature tables.\n",
    "    \n",
    "    Args:\n",
    "        theta_table_names:      list of all theta dataframe names, where features are in [3:52] columns,\n",
    "                                column 1 - sentences, column 2 - labels.\n",
    "        theta_dir:              set root dir for theta tables, if they are not in the same dir \n",
    "                                as the notebook. Otherwise, leave as is.\n",
    "        save_log_name:          filename to save the logs. You can mention the relative/absolute path as well,\n",
    "                                but make sure the directory exists on disk or elsewise an error might pop up\n",
    "    '''\n",
    "    clear_stdin()\n",
    "    bulk_results = []\n",
    "    \n",
    "    for theta in tqdm(theta_table_names):\n",
    "        features = pd.read_csv(theta_dir+theta, index_col=None)\n",
    "        \n",
    "        X = features.iloc[:, 3:53].values.tolist()\n",
    "        y = features['targets']\n",
    "        # X = df.values.tolist()    # if it is only theta table\n",
    "        \n",
    "        train_results = train(X, y)\n",
    "        model_results = scores(train_results, mode='return')\n",
    "        bulk_results.append(f'{theta}\\n{model_results}')\n",
    "        \n",
    "    \n",
    "    with open(save_log_name, 'wt', encoding='utf-8') as f:\n",
    "        for i in bulk_results:\n",
    "            print(i, file=f)\n",
    "            print('='*30, file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nums=[40,50,60,70,80,90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:08<00:00, 11.46s/it]\n"
     ]
    }
   ],
   "source": [
    "bulk_train(theta_table_names=['metcorp_tm_sparse{}.csv'.format(i) for i in topic_nums], theta_dir='./artm/thetas/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milti-feature Classifier\n",
    "### TM + lex / morph / concr-absrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ast, csv\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from metcorp_utils import compute_statistics, assign_scores, freq_table   # custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes that were eliminated from metaphor corpus\n",
    "emptied_indexes = [325, 379, 417, 909, 914, 1067, 1146, 1193, 1214, 1301, 1325, 1393, 1398, 1412, 1826,\n",
    "                   1830, 1864, 1891, 2015, 2016, 2017, 2051, 2078, 2080, 2081, 2086, 2138, 2154, 2178, \n",
    "                   2229, 2296, 2425, 2945, 3116, 3128, 3437, 3685, 4036, 4182, 4183, 4770, 4809, 4928, \n",
    "                   4984, 5039, 5134, 5136, 5228, 5248, 5322, 5493, 5543, 6059, 6067, 6093, 6109, 6218, \n",
    "                   6232, 6288, 6301, 6461, 6663, 6769, 6924, 7136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>class</th>\n",
       "      <th>sentid</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos</th>\n",
       "      <th>concr</th>\n",
       "      <th>abstr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>бомбардировать#1</td>\n",
       "      <td>['время', 'ребенок', 'музыка']</td>\n",
       "      <td>['ADV', 'PART', ',', 'SPRO', 'PART', 'ADV', 'A...</td>\n",
       "      <td>0.246517</td>\n",
       "      <td>0.213053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>бомбардировать#2</td>\n",
       "      <td>['добрынин', 'говорить', 'шевченко', 'центр', ...</td>\n",
       "      <td>['S anim nom', 'ADV', 'V ipf praet indic', 'S ...</td>\n",
       "      <td>0.181383</td>\n",
       "      <td>0.199301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>бомбардировать#3</td>\n",
       "      <td>['принять', 'внимание', 'настойчиво', 'группа'...</td>\n",
       "      <td>['CONJ', 'V pf - inf', 'ВО', 'S inan acc', ','...</td>\n",
       "      <td>0.152103</td>\n",
       "      <td>0.233698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  class            sentid  \\\n",
       "0      0      1  бомбардировать#1   \n",
       "1      1      1  бомбардировать#2   \n",
       "2      2      1  бомбардировать#3   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0                     ['время', 'ребенок', 'музыка']   \n",
       "1  ['добрынин', 'говорить', 'шевченко', 'центр', ...   \n",
       "2  ['принять', 'внимание', 'настойчиво', 'группа'...   \n",
       "\n",
       "                                                 pos     concr     abstr  \n",
       "0  ['ADV', 'PART', ',', 'SPRO', 'PART', 'ADV', 'A...  0.246517  0.213053  \n",
       "1  ['S anim nom', 'ADV', 'V ipf praet indic', 'S ...  0.181383  0.199301  \n",
       "2  ['CONJ', 'V pf - inf', 'ВО', 'S inan acc', ','...  0.152103  0.233698  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and Delete rows that were eliminated when retrieving features for the metcorp (1-2 word ones).\n",
    "infile = r'../datasets/pos-lex-abstr.csv'\n",
    "df = pd.read_csv(infile, sep = ',', index_col = 0)\n",
    "# df = df.drop([df.index[i] for i in emptied_indexes]).reset_index()\n",
    "print(len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Concreteness / Abstractness features in Metaphor corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is concreteness score calculation process. If you already have the dataframe with calculated concreteness score, skip this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "concr = pd.read_csv('../datasets/concretness5.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>words_perf_verbs</th>\n",
       "      <th>words_imperf_verbs</th>\n",
       "      <th>things_concr_k10</th>\n",
       "      <th>abstr_concr_k10</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>гребенка_NOUN</td>\n",
       "      <td>гребенка_NOUN</td>\n",
       "      <td>0.300831</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>0.243805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>бюджет_NOUN</td>\n",
       "      <td>бюджет_NOUN</td>\n",
       "      <td>0.170042</td>\n",
       "      <td>0.259276</td>\n",
       "      <td>0.214659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>письмо_NOUN</td>\n",
       "      <td>письмо_NOUN</td>\n",
       "      <td>0.260119</td>\n",
       "      <td>0.216631</td>\n",
       "      <td>0.238375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>правительство_NOUN</td>\n",
       "      <td>правительство_NOUN</td>\n",
       "      <td>0.142006</td>\n",
       "      <td>0.263686</td>\n",
       "      <td>0.202846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>зарплата_NOUN</td>\n",
       "      <td>зарплата_NOUN</td>\n",
       "      <td>0.166051</td>\n",
       "      <td>0.247007</td>\n",
       "      <td>0.206529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    words_perf_verbs  words_imperf_verbs  things_concr_k10  \\\n",
       "0           0       гребенка_NOUN       гребенка_NOUN          0.300831   \n",
       "1           1         бюджет_NOUN         бюджет_NOUN          0.170042   \n",
       "2           2         письмо_NOUN         письмо_NOUN          0.260119   \n",
       "3           3  правительство_NOUN  правительство_NOUN          0.142006   \n",
       "4           4       зарплата_NOUN       зарплата_NOUN          0.166051   \n",
       "\n",
       "   abstr_concr_k10      mean  \n",
       "0         0.186778  0.243805  \n",
       "1         0.259276  0.214659  \n",
       "2         0.216631  0.238375  \n",
       "3         0.263686  0.202846  \n",
       "4         0.247007  0.206529  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'письмо_NOUN' in concr['words_perf_verbs'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemma = pd.read_csv('metcorp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_tags(text):\n",
    "    text = re.sub('_[A-Z]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemma['sents'] = [remove_tags(i) for i in pos_lemma['sents']]\n",
    "pos_lemma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemma.to_csv('lemma_targ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "concr['words_perf_verbs'] = [remove_tags(i) for i in concr['words_perf_verbs']]\n",
    "concr['words_imperf_verbs'] = [remove_tags(i) for i in concr['words_imperf_verbs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>words_perf_verbs</th>\n",
       "      <th>words_imperf_verbs</th>\n",
       "      <th>things_concr_k10</th>\n",
       "      <th>abstr_concr_k10</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>гребенка</td>\n",
       "      <td>гребенка</td>\n",
       "      <td>0.300831</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>0.243805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>бюджет</td>\n",
       "      <td>бюджет</td>\n",
       "      <td>0.170042</td>\n",
       "      <td>0.259276</td>\n",
       "      <td>0.214659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>письмо</td>\n",
       "      <td>письмо</td>\n",
       "      <td>0.260119</td>\n",
       "      <td>0.216631</td>\n",
       "      <td>0.238375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>правительство</td>\n",
       "      <td>правительство</td>\n",
       "      <td>0.142006</td>\n",
       "      <td>0.263686</td>\n",
       "      <td>0.202846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>зарплата</td>\n",
       "      <td>зарплата</td>\n",
       "      <td>0.166051</td>\n",
       "      <td>0.247007</td>\n",
       "      <td>0.206529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 words_perf_verbs words_imperf_verbs  things_concr_k10  \\\n",
       "0           0         гребенка           гребенка          0.300831   \n",
       "1           1           бюджет             бюджет          0.170042   \n",
       "2           2           письмо             письмо          0.260119   \n",
       "3           3    правительство      правительство          0.142006   \n",
       "4           4         зарплата           зарплата          0.166051   \n",
       "\n",
       "   abstr_concr_k10      mean  \n",
       "0         0.186778  0.243805  \n",
       "1         0.259276  0.214659  \n",
       "2         0.216631  0.238375  \n",
       "3         0.263686  0.202846  \n",
       "4         0.247007  0.206529  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17004171])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concr.loc[concr['words_perf_verbs'] == 'бюджет', 'things_concr_k10'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_concr(corpus, df):\n",
    "    \n",
    "    global_concr = []\n",
    "    global_abstr = []\n",
    "    \n",
    "    for seq in corpus:\n",
    "        local_concr = []\n",
    "        local_abstr = []\n",
    "        for word in seq.split():\n",
    "            if word in df['words_perf_verbs'].values:\n",
    "                local_concr.append(df.loc[df['words_perf_verbs'] == word, 'things_concr_k10'].values[0])\n",
    "                local_abstr.append(df.loc[df['words_perf_verbs'] == word, 'abstr_concr_k10'].values[0])\n",
    "            \n",
    "            elif word in df['words_imperf_verbs'].values:\n",
    "                local_concr.append(df.loc[df['words_imperf_verbs'] == word, 'things_concr_k10'].values[0])\n",
    "                local_abstr.append(df.loc[df['words_imperf_verbs'] == word, 'abstr_concr_k10'].values[0])\n",
    "                \n",
    "        global_concr.append(np.mean(local_concr))  \n",
    "        global_abstr.append(np.mean(local_abstr))\n",
    "    \n",
    "    return global_concr, global_abstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_concr, global_abstr = count_concr(list(pos_lemma['sents']), concr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concr'] = global_concr\n",
    "df['abstr'] = global_abstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pos-lex-abstr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>class</th>\n",
       "      <th>sentid</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos</th>\n",
       "      <th>concr</th>\n",
       "      <th>abstr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>бомбардировать#1</td>\n",
       "      <td>['время', 'ребенок', 'музыка']</td>\n",
       "      <td>['ADV', 'PART', ',', 'SPRO', 'PART', 'ADV', 'A...</td>\n",
       "      <td>0.246517</td>\n",
       "      <td>0.213053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>бомбардировать#2</td>\n",
       "      <td>['добрынин', 'говорить', 'шевченко', 'центр', ...</td>\n",
       "      <td>['S anim nom', 'ADV', 'V ipf praet indic', 'S ...</td>\n",
       "      <td>0.181383</td>\n",
       "      <td>0.199301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>бомбардировать#3</td>\n",
       "      <td>['принять', 'внимание', 'настойчиво', 'группа'...</td>\n",
       "      <td>['CONJ', 'V pf - inf', 'ВО', 'S inan acc', ','...</td>\n",
       "      <td>0.152103</td>\n",
       "      <td>0.233698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  class            sentid  \\\n",
       "0      0      1  бомбардировать#1   \n",
       "1      1      1  бомбардировать#2   \n",
       "2      2      1  бомбардировать#3   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0                     ['время', 'ребенок', 'музыка']   \n",
       "1  ['добрынин', 'говорить', 'шевченко', 'центр', ...   \n",
       "2  ['принять', 'внимание', 'настойчиво', 'группа'...   \n",
       "\n",
       "                                                 pos     concr     abstr  \n",
       "0  ['ADV', 'PART', ',', 'SPRO', 'PART', 'ADV', 'A...  0.246517  0.213053  \n",
       "1  ['S anim nom', 'ADV', 'V ipf praet indic', 'S ...  0.181383  0.199301  \n",
       "2  ['CONJ', 'V pf - inf', 'ВО', 'S inan acc', ','...  0.152103  0.233698  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-multilingual-cased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor(tokenizer.encode(i, add_special_tokens=True)) \n",
    "                  for i in list(pos_lemma['sents'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7077/7077 [01:05<00:00, 108.87it/s]\n"
     ]
    }
   ],
   "source": [
    "clear_stdin()\n",
    "sentence_embeddings = []\n",
    "\n",
    "for i in tqdm(input_ids):\n",
    "    _, outputs = model(i.unsqueeze(0).to(device))\n",
    "    sentence_embeddings.append(torch.mean(outputs[0], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([i.tolist() for i in sentence_embeddings]).to_csv('bert_embeds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(outputs[1][0], dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_df_1 = pd.read_csv('./theta_matrices/metcorp_tm_lda80.csv', index_col=None)\n",
    "theta_df_2 = pd.read_csv('./theta_matrices/metcorp_tm_sparse_80.csv', index_col=None)\n",
    "theta_df_3 = pd.read_csv('./theta_matrices/metcorp_tm_dense_80.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "concr = pd.read_csv('../datasets/pos-lex-abstr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_dict = ['бомбардировать', 'доить', 'греть', 'нападать', \n",
    "             'очертить', 'отрубить', 'пилить', \n",
    "             'подхватывать', 'причесать', 'распылять', \n",
    "             'разбавлять', 'съедать', 'трубить', 'уколоть', \n",
    "             'утюжить', 'выкраивать', 'взорвать', \n",
    "             'взвесить', 'зажигать', 'жонглировать']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embs_df = pd.read_csv('../datasets/bert_embeds.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embs = np.array(list(bert_embs_df[\"0\"].apply(ast.literal_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_embs = np.array([i.cpu().tolist() for i in sentence_embeddings]).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7077, 768)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/pos-lex-abstr.csv')\n",
    "\n",
    "y = df['class'].values\n",
    "\n",
    "X_lex = df['lemmas']\n",
    "X_pos = df['pos']\n",
    "X_conc = concr[['concr', 'abstr']].values\n",
    "\n",
    "X_tm_1 = theta_df_1.iloc[:, 3:].values\n",
    "X_tm_2 = theta_df_2.iloc[:, 3:].values\n",
    "X_tm_3 = theta_df_3.iloc[:, 3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7077, 7077, 7077, 7077)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y), len(X_lex), len(X_pos), len(theta_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "a = kf.split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), array([0]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'newkey': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d = {}\n",
    "d.setdefault('newkey',[]).extend([1,1,1])\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_list, test_list, clf, train_fold_labels):\n",
    "    train = pd.concat(train_list, axis=1)\n",
    "    test = pd.concat(test_list, axis=1)\n",
    "    pipeline = Pipeline([('scaler', MinMaxScaler()), ('clf', clf)])\n",
    "    pipeline.fit(train, train_fold_labels)\n",
    "    predictions = pipeline.predict(test)\n",
    "#     m = pickle.dumps(pipeline)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(X_lex, X_pos, X_conc, y, X_emb, return_results=True,\n",
    "                  X_tm=False, skip_non_tm=False, verb_dict=None, k_folds=5, clf='svc'):\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=0)\n",
    "    all_test_accuracies = {}\n",
    "    all_results = {}\n",
    "    all_outcomes = {}\n",
    "    \n",
    "    results_table = {}  # dict with model outputs\n",
    "#     lexfeats = []\n",
    "    \n",
    "    y = y.ravel()\n",
    "    \n",
    "    X_lex = X_lex\n",
    "    X_pos = X_pos\n",
    "    X_conc = X_conc\n",
    "    X_emb = X_emb\n",
    "\n",
    "    alltrue = []\n",
    "    all_test_indices = []\n",
    "    \n",
    "    if not skip_non_tm:\n",
    "        allpredictions_lex = []\n",
    "        allpredictions_pos = []\n",
    "        allpredictions_conc = []\n",
    "        allpredictions_emb = []\n",
    "        allpredictions_lex_pos = []\n",
    "        allpredictions_lex_conc = []\n",
    "        allpredictions_lex_emb = []\n",
    "        allpredictions_lex_pos_conc = []\n",
    "        allpredictions_lex_pos_emb = []\n",
    "        allpredictions_lex_emb_conc = []\n",
    "        allpredictions_lex_pos_emb_conc = []\n",
    "    \n",
    "    if X_tm is not None:\n",
    "        X_tm = X_tm\n",
    "        allpredictions_lex_tm = []\n",
    "        allpredictions_pos_tm = []\n",
    "        allpredictions_emb_tm = []\n",
    "        allpredictions_conc_tm = []\n",
    "        allpredictions_lex_emb_tm = []\n",
    "        allpredictions_lex_pos_tm = []\n",
    "        allpredictions_lex_conc_tm = []\n",
    "        allpredictions_emb_conc_tm = []\n",
    "        allpredictions_lex_pos_conc_tm = []\n",
    "        allpredictions_lex_pos_emb_tm = []\n",
    "        allpredictions_lex_pos_conc_emb_tm = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_lex, y):\n",
    "        lex_train_fold, lex_test_fold = X_lex[train_index], X_lex[test_index]\n",
    "        lex_train_fold, lex_test_fold = [ast.literal_eval(x) for x in lex_train_fold], [ast.literal_eval(x) for x in lex_test_fold]\n",
    "\n",
    "        pos_train_fold, pos_test_fold = X_pos[train_index], X_pos[test_index]\n",
    "        pos_train_fold, pos_test_fold = [ast.literal_eval(x) for x in pos_train_fold], [ast.literal_eval(x) for x in pos_test_fold]\n",
    "        \n",
    "        conc_train, conc_test = X_conc[train_index], X_conc[test_index]\n",
    "        emb_train, emb_test = X_emb[train_index], X_emb[test_index]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        emb_train, emb_test = scaler.fit_transform(emb_train), scaler.transform(emb_test)\n",
    "        \n",
    "        if X_tm is not None:\n",
    "            tm_train_fold, tm_test_fold = X_tm[train_index], X_tm[test_index]\n",
    "\n",
    "        train_fold_labels, test_fold_labels = y[train_index], y[test_index]\n",
    "        alltrue += test_fold_labels.tolist()\n",
    "        \n",
    "        all_test_indices += list(test_index)\n",
    "        \n",
    "        lex_train_pairs = list(zip(train_fold_labels, lex_train_fold))\n",
    "        lex_test_pairs = list(zip(test_fold_labels, lex_test_fold))\n",
    "\n",
    "        lex_train_freq_table = freq_table(lex_train_pairs, verb_dict)  \n",
    "        lex_frequencies = lex_train_freq_table[0]\n",
    "        lex_met_corpus_size = lex_train_freq_table[1]\n",
    "        lex_nonmet_corpus_size = lex_train_freq_table[2]\n",
    "\n",
    "        lex_train_statistics = compute_statistics(lex_frequencies, lex_met_corpus_size, \n",
    "                                                   lex_nonmet_corpus_size)\n",
    "\n",
    "        lex_train = assign_scores(lex_train_fold, lex_train_statistics)\n",
    "        lex_test = assign_scores(lex_test_fold, lex_train_statistics)\n",
    "#         lexfeats.append(lex_test)\n",
    "\n",
    "        pos_train_pairs = list(zip(train_fold_labels, pos_train_fold))\n",
    "        pos_test_pairs = list(zip(test_fold_labels, pos_test_fold))\n",
    "\n",
    "        pos_train_freq_table = freq_table(pos_train_pairs, verb_dict)        \n",
    "        pos_frequencies = pos_train_freq_table[0]\n",
    "        pos_met_corpus_size = pos_train_freq_table[1]\n",
    "        pos_nonmet_corpus_size = pos_train_freq_table[2]\n",
    "\n",
    "        pos_train_statistics = compute_statistics(pos_frequencies, pos_met_corpus_size, \n",
    "                                                   pos_nonmet_corpus_size)\n",
    "\n",
    "        pos_train = assign_scores(pos_train_fold, pos_train_statistics)\n",
    "        pos_test = assign_scores(pos_test_fold, pos_train_statistics)\n",
    "\n",
    "        if not skip_non_tm:\n",
    "            if clf == 'svc':\n",
    "                clf_lex = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_pos = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_conc = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_emb = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_pos = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_conc = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_emb = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_pos_conc = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_pos_emb = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_emb_conc = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_pos_emb_conc = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "            elif clf == 'nn':\n",
    "                clf_lex = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')       \n",
    "                clf_pos = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_conc = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_emb = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_pos = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_conc = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_emb = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_pos_conc = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_pos_emb = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_emb_conc = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_pos_emb_conc = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                \n",
    "            elif clf == 'logreg':\n",
    "                clf_lex = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)       \n",
    "                clf_pos = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_conc = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_emb = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_lex_pos = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_lex_conc = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_lex_emb = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_lex_pos_conc = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_lex_pos_emb = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_lex_emb_conc = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                clf_lex_pos_emb_conc = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)  \n",
    "                \n",
    "            # LEX\n",
    "            clf_lex.fit(lex_train, train_fold_labels)\n",
    "            predictions_lex = clf_lex.predict(lex_test)\n",
    "            results_table.setdefault(nameof(clf_lex),[]).extend(predictions_lex)\n",
    "\n",
    "            # POS\n",
    "            clf_pos.fit(pos_train, train_fold_labels)\n",
    "            predictions_pos = clf_pos.predict(pos_test)\n",
    "            results_table.setdefault(nameof(clf_pos),[]).extend(predictions_pos)\n",
    "\n",
    "            # CONC\n",
    "            clf_conc.fit(conc_train, train_fold_labels)\n",
    "            predictions_conc = clf_conc.predict(conc_test)\n",
    "            results_table.setdefault(nameof(clf_conc),[]).extend(predictions_conc)\n",
    "\n",
    "            # EMB\n",
    "            clf_emb.fit(emb_train, train_fold_labels)\n",
    "            predictions_emb = clf_emb.predict(emb_test)\n",
    "            results_table.setdefault(nameof(clf_emb),[]).extend(predictions_emb)\n",
    "\n",
    "            # LEX + POS\n",
    "            predictions_lex_pos = train_and_predict([pd.DataFrame(lex_train), pd.DataFrame(pos_train)],\n",
    "                                                           (pd.DataFrame(lex_test), pd.DataFrame(pos_test)),\n",
    "                                                           clf_lex_pos, train_fold_labels=train_fold_labels)\n",
    "            results_table.setdefault(nameof(clf_lex_pos),[]).extend(predictions_lex_pos)\n",
    "            \n",
    "            # LEX + CONC\n",
    "            predictions_lex_conc = train_and_predict([pd.DataFrame(lex_train), pd.DataFrame(conc_train)],\n",
    "                                                           (pd.DataFrame(lex_test), pd.DataFrame(conc_test)),\n",
    "                                                           clf_lex_conc, train_fold_labels=train_fold_labels)\n",
    "            results_table.setdefault(nameof(clf_lex_conc),[]).extend(predictions_lex_conc)\n",
    "\n",
    "            # LEX + EMB\n",
    "            predictions_lex_emb = train_and_predict([pd.DataFrame(lex_train), pd.DataFrame(emb_train)],\n",
    "                                                           (pd.DataFrame(lex_test), pd.DataFrame(emb_test)),\n",
    "                                                           clf_lex_emb, train_fold_labels=train_fold_labels)\n",
    "            results_table.setdefault(nameof(clf_lex_emb),[]).extend(predictions_lex_emb)\n",
    "\n",
    "            # LEX + POS + CONC\n",
    "            predictions_lex_pos_conc = train_and_predict([pd.DataFrame(lex_train), pd.DataFrame(pos_train), \n",
    "                                                     pd.DataFrame(conc_train)], (pd.DataFrame(lex_test), \n",
    "                                                     pd.DataFrame(pos_test), pd.DataFrame(conc_test)),\n",
    "                                                     clf_lex_pos_conc, train_fold_labels=train_fold_labels)\n",
    "            results_table.setdefault(nameof(clf_lex_pos_conc),[]).extend(predictions_lex_pos_conc)\n",
    "            \n",
    "            # LEX + POS + EMB\n",
    "            predictions_lex_pos_emb = train_and_predict([pd.DataFrame(lex_train), pd.DataFrame(pos_train), \n",
    "                                                     pd.DataFrame(emb_train)], (pd.DataFrame(lex_test), \n",
    "                                                     pd.DataFrame(pos_test), pd.DataFrame(emb_test)),\n",
    "                                                     clf_lex_pos_emb, train_fold_labels=train_fold_labels)\n",
    "            results_table.setdefault(nameof(clf_lex_pos_emb),[]).extend(predictions_lex_pos_emb)\n",
    "\n",
    "            # LEX + EMB + CONC\n",
    "            predictions_lex_emb_conc = train_and_predict([pd.DataFrame(lex_train), pd.DataFrame(emb_train), \n",
    "                                                     pd.DataFrame(conc_train)], (pd.DataFrame(lex_test), \n",
    "                                                     pd.DataFrame(emb_test), pd.DataFrame(conc_test)),\n",
    "                                                     clf_lex_emb_conc, train_fold_labels=train_fold_labels)\n",
    "            results_table.setdefault(nameof(clf_lex_emb_conc),[]).extend(predictions_lex_emb_conc)\n",
    "            \n",
    "            # LEX + EMB + CONC\n",
    "            predictions_lex_pos_emb_conc = train_and_predict([pd.DataFrame(lex_train), pd.DataFrame(pos_train), \n",
    "                                                     pd.DataFrame(emb_train), pd.DataFrame(conc_train)], \n",
    "                                                     (pd.DataFrame(lex_test), pd.DataFrame(pos_test),\n",
    "                                                     pd.DataFrame(emb_test), pd.DataFrame(conc_test)),\n",
    "                                                     clf_lex_pos_emb_conc, train_fold_labels=train_fold_labels)\n",
    "            results_table.setdefault(nameof(clf_lex_pos_emb_conc),[]).extend(predictions_lex_pos_emb_conc)\n",
    "\n",
    "            allpredictions_lex += list(predictions_lex)\n",
    "            allpredictions_pos += list(predictions_pos)\n",
    "            allpredictions_emb += list(predictions_emb)\n",
    "            allpredictions_conc += list(predictions_conc)\n",
    "            allpredictions_lex_pos += list(predictions_lex_pos)\n",
    "            allpredictions_lex_conc += list(predictions_lex_conc)\n",
    "            allpredictions_lex_emb += list(predictions_lex_emb)\n",
    "            allpredictions_lex_pos_conc += list(predictions_lex_pos_conc)\n",
    "            allpredictions_lex_pos_emb += list(predictions_lex_pos_emb)\n",
    "            allpredictions_lex_emb_conc += list(predictions_lex_emb_conc)\n",
    "            allpredictions_lex_pos_emb_conc += list(predictions_lex_pos_emb_conc)\n",
    "        \n",
    "        if X_tm is not None:\n",
    "            clf_lex_tm = clf\n",
    "            clf_pos_tm = clf\n",
    "            clf_emb_tm = clf\n",
    "            clf_conc_tm = clf\n",
    "            clf_lex_pos_tm = clf\n",
    "            clf_lex_emb_tm = clf\n",
    "            clf_lex_conc_tm = clf\n",
    "            clf_conc_emb_tm = clf\n",
    "            clf_lex_pos_conc_tm = clf\n",
    "            clf_lex_pos_conc_emb_tm = clf\n",
    "            \n",
    "            if clf == 'svc':\n",
    "                clf_lex_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_pos_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_emb_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_conc_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_pos_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_emb_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_conc_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_conc_emb_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)         \n",
    "                clf_lex_pos_conc_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "                clf_lex_pos_conc_emb_tm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, \n",
    "                                    C=1000, multi_class='ovr', random_state=0)       \n",
    "            elif clf == 'nn':\n",
    "                clf_lex_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')       \n",
    "                clf_pos_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_emb_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_conc_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_pos_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_emb_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_emb_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_conc_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_conc_emb_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_pos_conc_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                clf_lex_pos_conc_emb_tm = MLPClassifier(alpha=0.1, max_iter=5000, learning_rate='adaptive')\n",
    "                \n",
    "            elif clf == 'logreg':\n",
    "                clf_lex_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)         \n",
    "                clf_pos_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_emb_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_conc_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_lex_pos_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_lex_emb_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_lex_emb_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_lex_conc_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_conc_emb_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_lex_pos_conc_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "                clf_lex_pos_conc_emb_tm = LogisticRegression(class_weight='balanced', solver='liblinear', \n",
    "                                             fit_intercept=True, max_iter=10000)\n",
    "            \n",
    "            # LEX + TM\n",
    "            clf_lex_tm.fit(lex_train.join(pd.DataFrame(tm_train_fold), rsuffix='tm'), train_fold_labels)\n",
    "            predictions_lex_tm = clf_lex_tm.predict(lex_test.join(pd.DataFrame(tm_test_fold), rsuffix='tm'))\n",
    "            results_table.setdefault(nameof(clf_lex_tm),[]).extend(predictions_lex_tm)\n",
    "            \n",
    "            # POS + TM\n",
    "            clf_pos_tm.fit(pos_train.join(pd.DataFrame(tm_train_fold), rsuffix='tm'), train_fold_labels)\n",
    "            predictions_pos_tm = clf_pos_tm.predict(pos_test.join(pd.DataFrame(tm_train_fold), rsuffix='tm'))\n",
    "            results_table.setdefault(nameof(clf_pos_tm),[]).extend(predictions_pos_tm)\n",
    "            \n",
    "            # CONC + TM\n",
    "            df_conc_tm_train = pd.DataFrame(conc_train).join(pd.DataFrame(tm_train_fold), rsuffix='tm')\n",
    "            df_conc_tm_test = pd.DataFrame(conc_test).join(pd.DataFrame(tm_test_fold), rsuffix='tm')\n",
    "                                                           \n",
    "            clf_conc_tm.fit(df_conc_tm_train, train_fold_labels)\n",
    "            predictions_conc_tm = clf_conc_tm.predict(df_conc_tm_test)\n",
    "            results_table.setdefault(nameof(clf_conc_tm),[]).extend(predictions_conc_tm)\n",
    "            \n",
    "            # EMB + TM\n",
    "            df_emb_tm_train = pd.DataFrame(emb_train).join(pd.DataFrame(tm_train_fold), rsuffix='tm')\n",
    "            df_emb_tm_test = pd.DataFrame(emb_test).join(pd.DataFrame(tm_test_fold), rsuffix='tm')\n",
    "            \n",
    "            clf_emb_tm.fit(df_emb_tm_train, train_fold_labels) \n",
    "            predictions_emb_tm = clf_emb_tm.predict(df_emb_tm_test)\n",
    "            results_table.setdefault(nameof(clf_emb_tm),[]).extend(predictions_emb_tm)\n",
    "            \n",
    "            # LEX + POS + TM \n",
    "            df_lex_pos_train = lex_train.join(pd.DataFrame(pos_train), rsuffix='pos')\n",
    "            df_lex_pos_test = lex_test.join(pd.DataFrame(pos_test), rsuffix='pos')\n",
    "            \n",
    "            df_lex_pos_tm_train = df_lex_pos_train.join(pd.DataFrame(tm_train_fold), rsuffix='tm')\n",
    "            df_lex_pos_tm_test = df_lex_pos_test.join(pd.DataFrame(tm_test_fold), rsuffix='tm')\n",
    "            \n",
    "            clf_lex_pos_tm.fit(df_lex_pos_tm_train, train_fold_labels)\n",
    "            predictions_lex_pos_tm = clf_lex_pos_tm.predict(df_lex_pos_tm_test)\n",
    "            results_table.setdefault(nameof(clf_lex_pos_tm),[]).extend(predictions_lex_pos_tm)\n",
    "            \n",
    "            # LEX + EMB + TM\n",
    "            df_lex_emb_train = lex_train.join(pd.DataFrame(emb_train), rsuffix='emb')\n",
    "            df_lex_emb_test = lex_test.join(pd.DataFrame(emb_test), rsuffix='emb')\n",
    "            \n",
    "            df_lex_emb_tm_train = df_lex_emb_train.join(pd.DataFrame(tm_train_fold), rsuffix='tm')\n",
    "            df_lex_emb_tm_test = df_lex_emb_test.join(pd.DataFrame(tm_test_fold), rsuffix='tm')\n",
    "            \n",
    "            clf_lex_emb_tm.fit(df_lex_emb_tm_train, train_fold_labels)\n",
    "            predictions_lex_emb_tm = clf_lex_emb_tm.predict(df_lex_emb_tm_test)\n",
    "            results_table.setdefault(nameof(clf_lex_emb_tm),[]).extend(predictions_lex_emb_tm)\n",
    "            \n",
    "            # LEX + CONC + TM\n",
    "            \n",
    "            df_lex_conc_tm_train = df_conc_tm_train.join(lex_train, rsuffix='lex')\n",
    "            df_lex_conc_tm_test = df_conc_tm_test.join(lex_test, rsuffix='lex')\n",
    "            \n",
    "            clf_lex_conc_tm.fit(df_lex_conc_tm_train, train_fold_labels)\n",
    "            predictions_lex_conc_tm = clf_lex_conc_tm.predict(df_lex_conc_tm_test)\n",
    "            results_table.setdefault(nameof(clf_lex_conc_tm),[]).extend(predictions_lex_conc_tm)\n",
    "            \n",
    "            # CONC + EMB + TM\n",
    "            \n",
    "            df_conc_emb_tm_train = df_emb_tm_train.join(lex_train, rsuffix='lex')\n",
    "            df_conc_emb_tm_test = df_emb_tm_test.join(lex_train, rsuffix='lex')\n",
    "            \n",
    "            clf_conc_emb_tm.fit(df_conc_emb_tm_train, train_fold_labels)\n",
    "            predictions_conc_emb_tm = clf_conc_emb_tm.predict(df_conc_emb_tm_test)\n",
    "            results_table.setdefault(nameof(clf_conc_emb_tm),[]).extend(predictions_conc_emb_tm)\n",
    "            \n",
    "            # LEX + POS + CONC + TM\n",
    "            \n",
    "            df_lex_pos_conc_tm_train = df_lex_pos_tm_train.join(pd.DataFrame(conc_train), rsuffix='conc')\n",
    "            df_lex_pos_conc_tm_test = df_lex_pos_tm_test.join(pd.DataFrame(conc_test), rsuffix='conc')\n",
    "            \n",
    "            clf_lex_pos_conc_tm.fit(df_lex_pos_conc_tm_train, train_fold_labels)\n",
    "            predictions_lex_pos_conc_tm = clf_lex_pos_conc_tm.predict(df_lex_pos_conc_tm_test)\n",
    "            results_table.setdefault(nameof(clf_lex_pos_conc_tm),[]).extend(predictions_lex_pos_conc_tm)\n",
    "            \n",
    "            # LEX + POS + CONC + EBM + TM\n",
    "            df_full_train = df_lex_pos_conc_tm_train.join(pd.DataFrame(emb_train), rsuffix='emb')\n",
    "            df_full_test = df_lex_pos_conc_tm_test.join(pd.DataFrame(emb_test), rsuffix='emb')\n",
    "            \n",
    "            clf_lex_pos_conc_emb_tm.fit(df_full_train, train_fold_labels)\n",
    "            predictions_lex_pos_conc_emb_tm = clf_lex_pos_conc_emb_tm.predict(df_full_test)\n",
    "            results_table.setdefault(nameof(clf_lex_pos_conc_emb_tm),[]).extend(predictions_lex_pos_conc_emb_tm)\n",
    "            \n",
    "            allpredictions_lex_tm += list(predictions_lex_tm)\n",
    "            allpredictions_pos_tm += list(predictions_pos_tm)\n",
    "            allpredictions_conc_tm += list(predictions_conc_tm)\n",
    "            allpredictions_emb_tm += list(predictions_emb_tm)\n",
    "            allpredictions_emb_conc_tm += list(predictions_conc_emb_tm)   #*\n",
    "            allpredictions_lex_emb_tm += list(predictions_lex_emb_tm)\n",
    "            allpredictions_lex_conc_tm += list(predictions_lex_conc_tm)\n",
    "            allpredictions_lex_pos_tm += list(predictions_lex_pos_tm)\n",
    "            allpredictions_lex_pos_conc_tm += list(predictions_lex_pos_conc_tm)\n",
    "            allpredictions_lex_pos_conc_emb_tm += list(predictions_lex_pos_conc_emb_tm)\n",
    "    \n",
    "    if not skip_non_tm:\n",
    "        accuracy_lex = accuracy_score(alltrue, allpredictions_lex)\n",
    "        accuracy_pos = accuracy_score(alltrue, allpredictions_pos)\n",
    "        accuracy_conc = accuracy_score(alltrue, allpredictions_conc)\n",
    "        accuracy_emb = accuracy_score(alltrue, allpredictions_emb)\n",
    "        accuracy_lex_conc = accuracy_score(alltrue, allpredictions_lex_conc)\n",
    "        accuracy_lex_emb = accuracy_score(alltrue, allpredictions_lex_emb)\n",
    "        accuracy_lex_pos = accuracy_score(alltrue, allpredictions_lex_pos)\n",
    "        accuracy_lex_pos_conc = accuracy_score(alltrue, allpredictions_lex_pos_conc)\n",
    "        accuracy_lex_pos_emb = accuracy_score(alltrue, allpredictions_lex_pos_emb)\n",
    "        accuracy_lex_emb_conc = accuracy_score(alltrue, allpredictions_lex_emb_conc)\n",
    "        accuracy_lex_pos_emb_conc = accuracy_score(alltrue, allpredictions_lex_pos_emb_conc)\n",
    "\n",
    "\n",
    "        print('Accuracy LEX: ', accuracy_lex)\n",
    "        print('Accuracy POS: ', accuracy_pos)\n",
    "        print('Accuracy CONC: ', accuracy_conc)\n",
    "        print('Accuracy EMB: ', accuracy_emb)\n",
    "        print('Accuracy LEX+EMB: ', accuracy_lex_emb)\n",
    "        print('Accuracy LEX+POS: ', accuracy_lex_pos)\n",
    "        print('Accuracy LEX+CONC: ', accuracy_lex_conc)\n",
    "        print('Accuracy LEX+POS+CONC: ', accuracy_lex_pos_conc)\n",
    "        print('Accuracy LEX+POS+EMB: ', accuracy_lex_pos_emb)\n",
    "        print('Accuracy LEX+EMB+CONC: ', accuracy_lex_emb_conc)\n",
    "        print('Accuracy LEX+POS+EMB+CONC: ', accuracy_lex_pos_emb_conc)\n",
    "    \n",
    "    \n",
    "    if X_tm is not None:\n",
    "        \n",
    "        accuracy_lex_tm = accuracy_score(alltrue, allpredictions_lex_tm)\n",
    "        accuracy_pos_tm = accuracy_score(alltrue, allpredictions_pos_tm)\n",
    "        accuracy_conc_tm = accuracy_score(alltrue, allpredictions_conc_tm)\n",
    "        accuracy_emb_tm = accuracy_score(alltrue, allpredictions_emb_tm)\n",
    "        accuracy_lex_emb_tm = accuracy_score(alltrue, allpredictions_lex_emb_tm)\n",
    "        accuracy_emb_conc_tm = accuracy_score(alltrue, allpredictions_emb_conc_tm)\n",
    "        accuracy_lex_pos_tm = accuracy_score(alltrue, allpredictions_lex_pos_tm)\n",
    "        accuracy_lex_conc_tm = accuracy_score(alltrue, allpredictions_lex_conc_tm)\n",
    "        accuracy_lex_pos_conc_tm = accuracy_score(alltrue, allpredictions_lex_pos_conc_tm)\n",
    "        accuracy_lex_pos_conc_emb_tm = accuracy_score(alltrue, allpredictions_lex_pos_conc_emb_tm)\n",
    "        \n",
    "        \n",
    "        print('Accuracy LEX+TM: ', accuracy_lex_tm)\n",
    "        print('Accuracy POS+TM: ', accuracy_pos_tm)\n",
    "        print('Accuracy CONC+TM: ', accuracy_conc_tm)\n",
    "        print('Accuracy EMB+TM: ', accuracy_emb_tm)\n",
    "        print('Accuracy EMB+CONC+TM: ', accuracy_emb_conc_tm)\n",
    "        print('Accuracy LEX+EMB+TM: ', accuracy_lex_emb_tm)\n",
    "        print('Accuracy LEX+CONC+TM: ', accuracy_lex_conc_tm)      \n",
    "        print('Accuracy LEX+POS+TM: ', accuracy_lex_pos_tm)\n",
    "        print('Accuracy LEX+POS+CONC+TM: ', accuracy_lex_pos_conc_tm)\n",
    "        print('Accuracy LEX+POS+CONC+EMB+TM: ', accuracy_lex_pos_conc_emb_tm)\n",
    "    \n",
    "    if return_results:\n",
    "        return results_table #, lexfeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8206867316659602\n",
      "Accuracy POS:  0.6661014554189628\n",
      "Accuracy CONC:  0.7165465592765295\n",
      "Accuracy EMB:  0.7203617351985304\n",
      "Accuracy LEX+EMB:  0.8163063444962555\n",
      "Accuracy LEX+POS:  0.8242193019641091\n",
      "Accuracy LEX+CONC:  0.8367952522255193\n",
      "Accuracy LEX+POS+CONC:  0.8383495831567048\n",
      "Accuracy LEX+POS+EMB:  0.8204041260421082\n",
      "Accuracy LEX+EMB+CONC:  0.824501907587961\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8267627525787763\n",
      "Accuracy LEX+TM:  0.837784371909001\n",
      "Accuracy POS+TM:  0.5900805426027977\n",
      "Accuracy CONC+TM:  0.7699590221845415\n",
      "Accuracy EMB+TM:  0.7400028260562386\n",
      "Accuracy EMB+CONC+TM:  0.526635580048043\n",
      "Accuracy LEX+EMB+TM:  0.8205454288540341\n",
      "Accuracy LEX+CONC+TM:  0.844284301257595\n",
      "Accuracy LEX+POS+TM:  0.8406104281475202\n",
      "Accuracy LEX+POS+CONC+TM:  0.8483820828034477\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8228062738448495\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_1, verb_dict=verb_dict,\n",
    "                       clf='svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.concatenate(lexfeats)).to_csv('../datasets/lex_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['gold'] = list(df['class'])\n",
    "results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/svm_lda_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8206867316659602\n",
      "Accuracy POS:  0.6661014554189628\n",
      "Accuracy CONC:  0.7165465592765295\n",
      "Accuracy EMB:  0.7203617351985304\n",
      "Accuracy LEX+EMB:  0.8163063444962555\n",
      "Accuracy LEX+POS:  0.8242193019641091\n",
      "Accuracy LEX+CONC:  0.8367952522255193\n",
      "Accuracy LEX+POS+CONC:  0.8383495831567048\n",
      "Accuracy LEX+POS+EMB:  0.8204041260421082\n",
      "Accuracy LEX+EMB+CONC:  0.824501907587961\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8267627525787763\n",
      "Accuracy LEX+TM:  0.8360887381658895\n",
      "Accuracy POS+TM:  0.5975695916348736\n",
      "Accuracy CONC+TM:  0.7607743394093542\n",
      "Accuracy EMB+TM:  0.7305355376571994\n",
      "Accuracy EMB+CONC+TM:  0.5259290659884132\n",
      "Accuracy LEX+EMB+TM:  0.8185671894870707\n",
      "Accuracy LEX+CONC+TM:  0.8400452168998163\n",
      "Accuracy LEX+POS+TM:  0.8386321887805568\n",
      "Accuracy LEX+POS+CONC+TM:  0.8423060618906316\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.820969337289812\n"
     ]
    }
   ],
   "source": [
    "# ARTM sparse\n",
    "results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_2,\n",
    "                        verb_dict=verb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['gold'] = list(df['class'])\n",
    "results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/svm_sparse_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8206867316659602\n",
      "Accuracy POS:  0.6661014554189628\n",
      "Accuracy CONC:  0.7165465592765295\n",
      "Accuracy EMB:  0.7203617351985304\n",
      "Accuracy LEX+EMB:  0.8163063444962555\n",
      "Accuracy LEX+POS:  0.8242193019641091\n",
      "Accuracy LEX+CONC:  0.8367952522255193\n",
      "Accuracy LEX+POS+CONC:  0.8383495831567048\n",
      "Accuracy LEX+POS+EMB:  0.8204041260421082\n",
      "Accuracy LEX+EMB+CONC:  0.824501907587961\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8267627525787763\n",
      "Accuracy LEX+TM:  0.8399039140878903\n",
      "Accuracy POS+TM:  0.5951674438321323\n",
      "Accuracy CONC+TM:  0.7614808534689841\n",
      "Accuracy EMB+TM:  0.7325137770241628\n",
      "Accuracy EMB+CONC+TM:  0.5219725872544864\n",
      "Accuracy LEX+EMB+TM:  0.8170128585558852\n",
      "Accuracy LEX+CONC+TM:  0.8447082096933729\n",
      "Accuracy LEX+POS+TM:  0.844284301257595\n",
      "Accuracy LEX+POS+CONC+TM:  0.8489472940511517\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.821534548537516\n"
     ]
    }
   ],
   "source": [
    "# ARTM dense\n",
    "results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_3, verb_dict=verb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['gold'] = list(df['class'])\n",
    "results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/svm_dense_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8211106401017381\n",
      "Accuracy POS:  0.6670905751024445\n",
      "Accuracy CONC:  0.7156987424049739\n",
      "Accuracy EMB:  0.7325137770241628\n",
      "Accuracy LEX+EMB:  0.8550233149639678\n",
      "Accuracy LEX+POS:  0.8219584569732937\n",
      "Accuracy LEX+CONC:  0.8356648297301116\n",
      "Accuracy LEX+POS+CONC:  0.8380669775328529\n",
      "Accuracy LEX+POS+EMB:  0.8588384908859686\n",
      "Accuracy LEX+EMB+CONC:  0.8623710611841175\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8654797230464887\n",
      "Accuracy LEX+TM:  0.837501766285149\n",
      "Accuracy POS+TM:  0.5441571287268616\n",
      "Accuracy CONC+TM:  0.7652960293909848\n",
      "Accuracy EMB+TM:  0.7559700438038717\n",
      "Accuracy EMB+CONC+TM:  0.5239508266214498\n",
      "Accuracy LEX+EMB+TM:  0.8588384908859686\n",
      "Accuracy LEX+CONC+TM:  0.8400452168998163\n",
      "Accuracy LEX+POS+TM:  0.837784371909001\n",
      "Accuracy LEX+POS+CONC+TM:  0.8396213084640385\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8599689133813763\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "logreg_results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_1,\n",
    "                               verb_dict=verb_dict, clf='logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_results['gold'] = list(df['class'])\n",
    "logreg_results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/logreg_lda_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8211106401017381\n",
      "Accuracy POS:  0.6670905751024445\n",
      "Accuracy CONC:  0.7156987424049739\n",
      "Accuracy EMB:  0.7325137770241628\n",
      "Accuracy LEX+EMB:  0.8550233149639678\n",
      "Accuracy LEX+POS:  0.8219584569732937\n",
      "Accuracy LEX+CONC:  0.8356648297301116\n",
      "Accuracy LEX+POS+CONC:  0.8380669775328529\n",
      "Accuracy LEX+POS+EMB:  0.8588384908859686\n",
      "Accuracy LEX+EMB+CONC:  0.8623710611841175\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8654797230464887\n",
      "Accuracy LEX+TM:  0.8372191606612972\n",
      "Accuracy POS+TM:  0.5438745231030098\n",
      "Accuracy CONC+TM:  0.7493288116433517\n",
      "Accuracy EMB+TM:  0.7455136357213509\n",
      "Accuracy EMB+CONC+TM:  0.5242334322453017\n",
      "Accuracy LEX+EMB+TM:  0.8568602515190052\n",
      "Accuracy LEX+CONC+TM:  0.8387734915924827\n",
      "Accuracy LEX+POS+TM:  0.8379256747209269\n",
      "Accuracy LEX+POS+CONC+TM:  0.8391974000282606\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.857708068390561\n"
     ]
    }
   ],
   "source": [
    "# ARTM sparse\n",
    "logreg_results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_2,\n",
    "                               verb_dict=verb_dict, clf='logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_results['gold'] = list(df['class'])\n",
    "logreg_results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/logreg_sparse_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8211106401017381\n",
      "Accuracy POS:  0.6670905751024445\n",
      "Accuracy CONC:  0.7156987424049739\n",
      "Accuracy EMB:  0.7325137770241628\n",
      "Accuracy LEX+EMB:  0.8550233149639678\n",
      "Accuracy LEX+POS:  0.8219584569732937\n",
      "Accuracy LEX+CONC:  0.8356648297301116\n",
      "Accuracy LEX+POS+CONC:  0.8380669775328529\n",
      "Accuracy LEX+POS+EMB:  0.8588384908859686\n",
      "Accuracy LEX+EMB+CONC:  0.8623710611841175\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8654797230464887\n",
      "Accuracy LEX+TM:  0.8222410625971457\n",
      "Accuracy POS+TM:  0.6248410343365833\n",
      "Accuracy CONC+TM:  0.7308181432810513\n",
      "Accuracy EMB+TM:  0.7371767698177194\n",
      "Accuracy EMB+CONC+TM:  0.5211247703829306\n",
      "Accuracy LEX+EMB+TM:  0.8557298290235976\n",
      "Accuracy LEX+CONC+TM:  0.8305779285007772\n",
      "Accuracy LEX+POS+TM:  0.8233714850925533\n",
      "Accuracy LEX+POS+CONC+TM:  0.8307192313127031\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8572841599547831\n"
     ]
    }
   ],
   "source": [
    "# ARTM dense\n",
    "logreg_results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_3, \n",
    "                               verb_dict=verb_dict, clf='logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_results['gold'] = list(df['class'])\n",
    "logreg_results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/logreg_dense_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8206867316659602\n",
      "Accuracy POS:  0.6669492722905186\n",
      "Accuracy CONC:  0.7171117705242335\n",
      "Accuracy EMB:  0.7209269464462342\n",
      "Accuracy LEX+EMB:  0.8397626112759644\n",
      "Accuracy LEX+POS:  0.8226649710329236\n",
      "Accuracy LEX+CONC:  0.8355235269181857\n",
      "Accuracy LEX+POS+CONC:  0.8391974000282606\n",
      "Accuracy LEX+POS+EMB:  0.8356648297301116\n",
      "Accuracy LEX+EMB+CONC:  0.8499364137346334\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8516320474777448\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "nn_results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=None, skip_non_tm=False, \n",
    "                           verb_dict=verb_dict, clf='nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results['gold'] = list(df['class'])\n",
    "nn_results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/nn_lda_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8202628232301823\n",
      "Accuracy POS:  0.6682209975978523\n",
      "Accuracy CONC:  0.718666101455419\n",
      "Accuracy EMB:  0.7391550091846828\n",
      "Accuracy LEX+EMB:  0.8415995478310019\n",
      "Accuracy LEX+POS:  0.824360604776035\n",
      "Accuracy LEX+CONC:  0.8358061325420376\n",
      "Accuracy LEX+POS+CONC:  0.8393387028401865\n",
      "Accuracy LEX+POS+EMB:  0.8431538787621874\n",
      "Accuracy LEX+EMB+CONC:  0.8420234562667797\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8242193019641091\n",
      "Accuracy LEX+TM:  0.8407517309594461\n",
      "Accuracy POS+TM:  0.5814610710753144\n",
      "Accuracy CONC+TM:  0.7627525787763176\n",
      "Accuracy EMB+TM:  0.7480570863360181\n",
      "Accuracy EMB+CONC+TM:  0.5274833969195987\n",
      "Accuracy LEX+EMB+TM:  0.8379256747209269\n",
      "Accuracy LEX+CONC+TM:  0.8432951815741133\n",
      "Accuracy LEX+POS+TM:  0.8418821534548537\n",
      "Accuracy LEX+POS+CONC+TM:  0.8435777871979653\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8523385615373746\n"
     ]
    }
   ],
   "source": [
    "# ARTM sparse\n",
    "nn_results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_2,\n",
    "                           verb_dict=verb_dict, clf='nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results['gold'] = list(df['class'])\n",
    "nn_results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/nn_sparse_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8205454288540341\n",
      "Accuracy POS:  0.6676557863501483\n",
      "Accuracy CONC:  0.718383495831567\n",
      "Accuracy EMB:  0.7335028967076445\n",
      "Accuracy LEX+EMB:  0.8389147944044086\n",
      "Accuracy LEX+POS:  0.8237953935283312\n",
      "Accuracy LEX+CONC:  0.8369365550374452\n",
      "Accuracy LEX+POS+CONC:  0.8383495831567048\n",
      "Accuracy LEX+POS+EMB:  0.827893175074184\n",
      "Accuracy LEX+EMB+CONC:  0.8572841599547831\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8394800056521124\n",
      "Accuracy LEX+TM:  0.8261975413310725\n",
      "Accuracy POS+TM:  0.615797654373322\n",
      "Accuracy CONC+TM:  0.7528613819415007\n",
      "Accuracy EMB+TM:  0.7368941641938674\n",
      "Accuracy EMB+CONC+TM:  0.5194291366398192\n",
      "Accuracy LEX+EMB+TM:  0.8270453582026283\n",
      "Accuracy LEX+CONC+TM:  0.8349583156704818\n",
      "Accuracy LEX+POS+TM:  0.8288822947576657\n",
      "Accuracy LEX+POS+CONC+TM:  0.8384908859686308\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8545994065281899\n"
     ]
    }
   ],
   "source": [
    "# ARTM dense\n",
    "nn_results = train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_3,\n",
    "                           verb_dict=verb_dict, clf='nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results['gold'] = list(df['class'])\n",
    "nn_results['lemmas'] = list(df['lemmas'])\n",
    "pd.DataFrame(results).to_csv('./clf_model_outputs/nn_dense_80.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Printing Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8164476473081814\n",
      "Accuracy POS:  0.675710046629928\n",
      "Accuracy CONC:  0.7172530733361594\n",
      "Accuracy EMB:  0.7195139183269748\n",
      "Accuracy LEX+EMB:  0.8074042673449202\n",
      "Accuracy LEX+POS:  0.8204041260421082\n",
      "Accuracy LEX+CONC:  0.8326974706796665\n",
      "Accuracy LEX+POS+CONC:  0.8352409212943338\n",
      "Accuracy LEX+POS+EMB:  0.8124911685742546\n",
      "Accuracy LEX+EMB+CONC:  0.8117846545146249\n",
      "Accuracy LEX+POS+EMB+CONC:  0.817578069803589\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=None, verb_dict=verb_dict, return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX+TM:  0.8321322594319627\n",
      "Accuracy POS+TM:  0.6149498375017662\n",
      "Accuracy CONC+TM:  0.7508831425745373\n",
      "Accuracy EMB+TM:  0.7255899392397909\n",
      "Accuracy EMB+CONC+TM:  0.5270594884838208\n",
      "Accuracy LEX+EMB+TM:  0.8124911685742546\n",
      "Accuracy LEX+CONC+TM:  0.8372191606612972\n",
      "Accuracy LEX+POS+TM:  0.8346757100466299\n",
      "Accuracy LEX+POS+CONC+TM:  0.8404691253355941\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8133389854458104\n"
     ]
    }
   ],
   "source": [
    "# ARTM sparse\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_2, skip_non_tm=True,\n",
    "              verb_dict=verb_dict, return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX+TM:  0.834534407234704\n",
      "Accuracy POS+TM:  0.5971456831990957\n",
      "Accuracy CONC+TM:  0.76162215628091\n",
      "Accuracy EMB+TM:  0.7315246573406811\n",
      "Accuracy EMB+CONC+TM:  0.523103009749894\n",
      "Accuracy LEX+EMB+TM:  0.8075455701568461\n",
      "Accuracy LEX+CONC+TM:  0.8400452168998163\n",
      "Accuracy LEX+POS+TM:  0.8394800056521124\n",
      "Accuracy LEX+POS+CONC+TM:  0.8452734209410767\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8117846545146249\n"
     ]
    }
   ],
   "source": [
    "# ARTM dense\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_3,\n",
    "              skip_non_tm=True, verb_dict=verb_dict, return_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8172954641797372\n",
      "Accuracy POS:  0.6748622297583722\n",
      "Accuracy CONC:  0.7158400452168998\n",
      "Accuracy EMB:  0.7309594460929772\n",
      "Accuracy LEX+EMB:  0.8483820828034477\n",
      "Accuracy LEX+POS:  0.8201215204182564\n",
      "Accuracy LEX+CONC:  0.8326974706796665\n",
      "Accuracy LEX+POS+CONC:  0.8349583156704818\n",
      "Accuracy LEX+POS+EMB:  0.8530450755970044\n",
      "Accuracy LEX+EMB+CONC:  0.854316800904338\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8584145824501908\n",
      "Accuracy LEX+TM:  0.8332626819273704\n",
      "Accuracy POS+TM:  0.5407658612406386\n",
      "Accuracy CONC+TM:  0.7477744807121661\n",
      "Accuracy EMB+TM:  0.7469266638406105\n",
      "Accuracy EMB+CONC+TM:  0.5256464603645613\n",
      "Accuracy LEX+EMB+TM:  0.8540341952804861\n",
      "Accuracy LEX+CONC+TM:  0.8359474353539635\n",
      "Accuracy LEX+POS+TM:  0.8331213791154444\n",
      "Accuracy LEX+POS+CONC+TM:  0.8369365550374452\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8558711318355235\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_1,\n",
    "              skip_non_tm=False, return_results=True,\n",
    "              verb_dict=verb_dict, clf='logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8172954641797372\n",
      "Accuracy POS:  0.6748622297583722\n",
      "Accuracy CONC:  0.7158400452168998\n",
      "Accuracy EMB:  0.7309594460929772\n",
      "Accuracy LEX+EMB:  0.8483820828034477\n",
      "Accuracy LEX+POS:  0.8201215204182564\n",
      "Accuracy LEX+CONC:  0.8326974706796665\n",
      "Accuracy LEX+POS+CONC:  0.8349583156704818\n",
      "Accuracy LEX+POS+EMB:  0.8530450755970044\n",
      "Accuracy LEX+EMB+CONC:  0.854316800904338\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8584145824501908\n",
      "Accuracy LEX+TM:  0.8319909566200367\n",
      "Accuracy POS+TM:  0.5517874805708634\n",
      "Accuracy CONC+TM:  0.7411332485516462\n",
      "Accuracy EMB+TM:  0.745372332909425\n",
      "Accuracy EMB+CONC+TM:  0.5259290659884132\n",
      "Accuracy LEX+EMB+TM:  0.8536102868447082\n",
      "Accuracy LEX+CONC+TM:  0.8348170128585559\n",
      "Accuracy LEX+POS+TM:  0.8324148650558146\n",
      "Accuracy LEX+POS+CONC+TM:  0.8346757100466299\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.854175498092412\n"
     ]
    }
   ],
   "source": [
    "# ARTM sparse\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_2,\n",
    "              skip_non_tm=False, return_results=True,\n",
    "              verb_dict=verb_dict, clf='logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8172954641797372\n",
      "Accuracy POS:  0.6748622297583722\n",
      "Accuracy CONC:  0.7158400452168998\n",
      "Accuracy EMB:  0.7309594460929772\n",
      "Accuracy LEX+EMB:  0.8483820828034477\n",
      "Accuracy LEX+POS:  0.8201215204182564\n",
      "Accuracy LEX+CONC:  0.8326974706796665\n",
      "Accuracy LEX+POS+CONC:  0.8349583156704818\n",
      "Accuracy LEX+POS+EMB:  0.8530450755970044\n",
      "Accuracy LEX+EMB+CONC:  0.854316800904338\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8584145824501908\n",
      "Accuracy LEX+TM:  0.8185671894870707\n",
      "Accuracy POS+TM:  0.6263953652677688\n",
      "Accuracy CONC+TM:  0.7312420517168292\n",
      "Accuracy EMB+TM:  0.7353398332626819\n",
      "Accuracy EMB+CONC+TM:  0.5221138900664123\n",
      "Accuracy LEX+EMB+TM:  0.8490885968630776\n",
      "Accuracy LEX+CONC+TM:  0.827893175074184\n",
      "Accuracy LEX+POS+TM:  0.8199802176063303\n",
      "Accuracy LEX+POS+CONC+TM:  0.8287409919457397\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8520559559135227\n"
     ]
    }
   ],
   "source": [
    "# ARTM dense\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_3, skip_non_tm=False, \n",
    "              verb_dict=verb_dict, clf='logreg', return_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX:  0.8171541613678113\n",
      "Accuracy POS:  0.6762752578776318\n",
      "Accuracy CONC:  0.7173943761480853\n",
      "Accuracy EMB:  0.7407093401158683\n",
      "Accuracy LEX+EMB:  0.8318496538081108\n",
      "Accuracy LEX+POS:  0.8199802176063303\n",
      "Accuracy LEX+CONC:  0.8332626819273704\n",
      "Accuracy LEX+POS+CONC:  0.834393104422778\n",
      "Accuracy LEX+POS+EMB:  0.844566906881447\n",
      "Accuracy LEX+EMB+CONC:  0.8384908859686308\n",
      "Accuracy LEX+POS+EMB+CONC:  0.8485233856153738\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=None, skip_non_tm=False, \n",
    "              verb_dict=verb_dict, clf='nn', return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX+TM:  0.831001836936555\n",
      "Accuracy POS+TM:  0.5954500494559841\n",
      "Accuracy CONC+TM:  0.7534265931892045\n",
      "Accuracy EMB+TM:  0.7346333192030522\n",
      "Accuracy EMB+CONC+TM:  0.5276246997315247\n",
      "Accuracy LEX+EMB+TM:  0.8362300409778155\n",
      "Accuracy LEX+CONC+TM:  0.8331213791154444\n",
      "Accuracy LEX+POS+TM:  0.8322735622438886\n",
      "Accuracy LEX+POS+CONC+TM:  0.8365126466016674\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8492298996750035\n"
     ]
    }
   ],
   "source": [
    "# ARTM sparse\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_2, skip_non_tm=True, \n",
    "              verb_dict=verb_dict, clf='nn', return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LEX+TM:  0.8230888794687015\n",
      "Accuracy POS+TM:  0.6133955065705807\n",
      "Accuracy CONC+TM:  0.7542744100607602\n",
      "Accuracy EMB+TM:  0.7103292355517875\n",
      "Accuracy EMB+CONC+TM:  0.5228204041260421\n",
      "Accuracy LEX+EMB+TM:  0.8399039140878903\n",
      "Accuracy LEX+CONC+TM:  0.8321322594319627\n",
      "Accuracy LEX+POS+TM:  0.8252084216475908\n",
      "Accuracy LEX+POS+CONC+TM:  0.8338278931750742\n",
      "Accuracy LEX+POS+CONC+EMB+TM:  0.8332626819273704\n"
     ]
    }
   ],
   "source": [
    "# ARTM dense\n",
    "train_process(X_lex, X_pos, X_conc, y, X_emb=bert_embs, X_tm=X_tm_3, skip_non_tm=True, \n",
    "              verb_dict=verb_dict, clf='nn', return_results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
