{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing options: Stanza and Pymorphy2\n",
    "*Anastasia Nikiforova. HSE, Computational Linguistics*\n",
    "\n",
    "*Thesis: Metaphor Identification using Topic Modeling*\n",
    "\n",
    "This notebook is only FYI, for the actual preprocessing pipeline refer to the Topic_modeling_on_wiki.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#Create lemmatizer and stopwords list \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords += ['ваш', 'наш', 'твой', 'который', 'это', 'то', 'что', 'кто', 'какой']\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Tokenization, lemmatization and POS-tagging with Stanza\n",
    "Second option - pymorphy2, which is significantly faster. Skip to the next section for pymorphy2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('ru')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "import pandas as pd    # in case if wasn't imported before\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lemmatizer and stopwords list \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords += ['ваш', 'наш', 'твой', 'который', 'это', 'то', 'что', 'кто', 'какой']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_stdin():\n",
    "    if hasattr(tqdm, '_instances'):\n",
    "        for instance in list(tqdm._instances):\n",
    "            tqdm._decr_instances(instance)\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    '''\n",
    "    Use Stanza to preprocess, pos-tag and lemmatize the corpus.\n",
    "    Save results to a text file after processing.\n",
    "    \n",
    "    Args:\n",
    "        corpus:   list of strings: [article 1, article 2.....]\n",
    "        \n",
    "    Output:\n",
    "         Tuple(Lemmas, POS-Tags, Lemma_POS) for eatch article. Choose what's relevamt for you.\n",
    "         The full output will be saved to 'lemma_pos_wiki_articles.txt' file.\n",
    "    '''\n",
    "    clear_stdin()\n",
    "\n",
    "    lemmas = []\n",
    "    pos = []\n",
    "    lemmas_pos = []\n",
    "    \n",
    "    with open('lemma_pos_wiki_articles.txt', 'w') as f:\n",
    "        for i in tqdm(corpus):\n",
    "            for sent in nlp(i).sentences:\n",
    "                lemmas.append(' '.join([word.lemma.lower() for word in sent.words\n",
    "                            if word.lemma.lower() not in russian_stopwords\n",
    "                            and word.lemma.lower().strip() not in punctuation]))\n",
    "                pos.append(' '.join([word.pos for word in sent.words \n",
    "                                   if word.lemma.lower() not in russian_stopwords\n",
    "                                   and word.lemma.lower().strip() not in punctuation]))\n",
    "                l_p = ' '.join([word.lemma.lower() + \"_\" + word.pos\n",
    "                                   for word in sent.words \n",
    "                                   if word.lemma.lower() not in russian_stopwords\n",
    "                                   and word.lemma.lower().strip() not in punctuation])\n",
    "                lemmas_pos.append(l_p)\n",
    "            \n",
    "                print(l_p, file=f)\n",
    "    \n",
    "    return lemmas, pos, lemmas_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_cleaned = [' '.join([t.lower() for t in seq.split() if t.lower() not in russian_stopwords]) for seq in wiki_vw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas, pos, lemmas_pos = preprocess_corpus(wiki_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['lemmas'] = lemmas\n",
    "df['pos'] = pos\n",
    "df['lemmas_pos'] = lemmas_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option: Pymorphy2 POS Tagger\n",
    "\n",
    "Для сравнения: Stanza обрабатывает 10 статей за 2 минуты, тогда как у Pymorphy2 на это уходит 5 секунд.\n",
    "На 50 тыс. статей Stanza необходимо около 120 часов на GPU, тогда как Pymorphy2 обрабатывает этот же корпус за 1,5 часа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_stdin():\n",
    "    if hasattr(tqdm, '_instances'):\n",
    "        for instance in list(tqdm._instances):\n",
    "            tqdm._decr_instances(instance)\n",
    "\n",
    "def pos_lemma_word(word):\n",
    "    if '<num' in word or word.isdecimal():\n",
    "        word_lemma = word\n",
    "        word_pos = 'NUMR'\n",
    "        word_lemma_pos = '_NUMR'\n",
    "    else:\n",
    "        res = morph.parse(word)[0]\n",
    "        if res.tag.POS and res.normal_form and len(res.tag.POS)>1 \\\n",
    "            and res.normal_form not in russian_stopwords:\n",
    "            word_lemma = res.normal_form\n",
    "            word_pos =res.tag.POS\n",
    "            word_lemma_pos = str(res.normal_form) + '_' + str(res.tag.POS)\n",
    "            \n",
    "        else:\n",
    "            word_lemma = None\n",
    "            word_pos = None\n",
    "            word_lemma_pos = None\n",
    "            \n",
    "    return word_lemma, word_pos, word_lemma_pos\n",
    "            \n",
    "def preprocess_pymorphy(corpus):\n",
    "    clear_stdin()\n",
    "\n",
    "    lemmas = []\n",
    "    pos = []\n",
    "    lemmas_pos = []\n",
    "    \n",
    "    #with open('lemma_pos_wiki_pymorphy.txt', 'w') as f:\n",
    "    for article in tqdm(corpus):\n",
    "        sent_lemmas = []\n",
    "        sent_pos = []\n",
    "        sent_lemma_pos = []\n",
    "\n",
    "        for word in article.split():\n",
    "            \n",
    "            word_lemma, word_pos, word_lemma_pos = pos_lemma_word(word)\n",
    "            \n",
    "            if word_lemma is not None:\n",
    "                sent_lemmas.append(word_lemma)\n",
    "                sent_pos.append(word_pos)\n",
    "                sent_lemma_pos.append(word_lemma_pos)\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        lemmas.append(' '.join(sent_lemmas))\n",
    "        pos.append(' '.join(sent_pos))\n",
    "        lemmas_pos.append(' '.join(sent_lemma_pos))\n",
    "            \n",
    "            #print(sent_lemma_pos, file=f)\n",
    "        #f.close()\n",
    "        \n",
    "    return lemmas, pos, lemmas_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_cleaned = [' '.join([t.lower() for t in seq.split() if t.lower() not in russian_stopwords]) for seq in wiki_vw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['lemmas'] = lemmas\n",
    "df['pos'] = pos\n",
    "df['lemmas_pos'] = lemmas_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wiki_articles_pymorphy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another option - count all word occurrences in the corpus and, thus, tag less and tag faster\n",
    "\n",
    "BigARTM receives texts in vowpal wabbit format.\n",
    "\n",
    "It means that:\n",
    "* Each text is presented as a bag-of-words.\n",
    "* A document can consist preprocessed words with repetitions or preprocessed words with the number of occurrences, like: \"|text this:1 word:3, is:4 repeated:2\".\n",
    "* Word order is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_c = [i for i in wiki_cleaned if len(i)>0]\n",
    "len(wiki_c), len(wiki_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter counts occurrences of items in the list and creates a dictionary\n",
    "wiki_counter = [dict(Counter(i.split())) for i in wiki_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemma_word('литва')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_wiki_counter = []\n",
    "for i, count_dict in enumerate(wiki_counter):\n",
    "    proc_count_dict = {pos_lemma_word(word)[2]: counts for word, counts in count_dict.items() \n",
    "                       if pos_lemma_word(word)[2] is not None}\n",
    "    tagged_wiki_counter.append(proc_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions = ['также_CONJ', 'б_PRCL', 'около_PREP', 'ещё_ADVB', 'э_INTJ', 'её_ADJF',\n",
    "              'мм_INTJ', 'однако_CONJ', 'например_CONJ', 'из-за_PREP', 'среди_PREP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vw_wiki_counts.txt', 'wt', encoding='utf-8') as f:\n",
    "    for article in tagged_wiki_counter:\n",
    "        text = \"|text \"\n",
    "        for k, v in article.items():\n",
    "            if k not in exclusions:\n",
    "                text += str(k) + ':' + str(v) + ' '\n",
    "        print(text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = open('vw_wiki_counts.txt', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
